# -*- coding: utf-8 -*-
"""MovieReviewSentimentClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18jptYiAdPzySeUB-fZB5u-kVQbdWiFMf

#Importing required libraries
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import re # for regex

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score
import pickle
from pandas import DataFrame as df
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

"""#Importing the IMDB 50k review dataset in .csv format"""

data = pd.read_csv('IMDB-Dataset.csv')
print("Shape of the DataFrame:",data.shape)
#data.head()
data.sample(5)

data.info()

"""# Visual representation of number of positive and negative reviews"""

print(data.sentiment.value_counts())
sns.countplot(data.sentiment, palette=['#2B9348',"#D62828"])

"""# Creating Wordcloud to see the most frequent words in the dataset"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

stopwords = set(STOPWORDS)
stopwords.update(["br", "href"])
textt = " ".join(review for review in data.review)
wordcloud = WordCloud(stopwords=stopwords, width=1600, height=800,max_font_size=200).generate(textt)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig('wordcloud.png')
plt.show()

"""# Replacing positive and negative review to binary digits 1 and 0 respectively"""

data.sentiment.replace('positive',1,inplace=True)
data.sentiment.replace('negative',0,inplace=True)
data.sample(10)

data.review[0]

"""# Preprocessing of Text"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords

#1. Remove HTML tags
def clean(text):
    cleaned = re.compile(r'<.*?>')
    return re.sub(cleaned,'',text)

data.review = data.review.apply(clean)
data.review[0]

#2. Remove special characters
def is_special(text):
    rem = ''
    for i in text:
        if i.isalnum():
            rem = rem + i
        else:
            rem = rem + ' '
    return rem

data.review = data.review.apply(is_special)
data.review[0]


#3. Convert everything to lowercase
def to_lower(text):
    return text.lower()

data.review = data.review.apply(to_lower)
data.review[0]


#4. Remove stopwords
def rem_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    return [w for w in words if w not in stop_words]

data.review = data.review.apply(rem_stopwords)
data.review[0]


#5. Stem the words
def stem_txt(text):
    ss = SnowballStemmer('english')
    return " ".join([ss.stem(w) for w in text])

data.review = data.review.apply(stem_txt)
data.review[0]

data.head()

"""100 most frequent words with count - Graphical representation"""

def freq_words(x, terms = 30): 
  all_words = ' '.join([text for text in x]) 
  all_words = all_words.split() 
  fdist = nltk.FreqDist(all_words) 
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) 
  
  # selecting top 20 most frequent words 
  d = words_df.nlargest(columns="count", n = terms) 
  
  # visualize words and frequencies
  plt.figure(figsize=(12,15)) 
  ax = sns.barplot(data=d, x= "count", y = "word") 
  ax.set(ylabel = 'Word') 
  plt.show()
  
# print 100 most frequent words 
freq_words(data['review'], 100)

X = np.array(data.iloc[:,0].values)
y = np.array(data.sentiment.values)
cv = CountVectorizer(max_features = 1000)
X = cv.fit_transform(data.review).toarray()
print("X.shape = ",X.shape)
print("y.shape = ",y.shape)

print(X)

"""# Train - Validation - Test datasplit = 80 - 10 - 10"""

#Train test split
train_size = 0.8
trainx, remx, trainy, remy = train_test_split(X,y, train_size=0.8)
test_size = 0.5
validx, testx, validy, testy = train_test_split(remx, remy, test_size=0.5)

print(trainx.shape), print(trainy.shape)
print(validx.shape), print(validy.shape)
print(testx.shape), print(testy.shape)

"""# Defining Machine Learning models and training them

# Gaussian Naive Bayes
"""

# Defining the models and Training them
modelgnb = GaussianNB()
print("Gaussian Naive Bayes:")
modelgnb.fit(trainx,trainy)
validpredict = modelgnb.predict(validx)
print("validation accuracy ", 100*accuracy_score(testy,validpredict),("%"))
predicty = modelgnb.predict(testx)
print("test accuracy ", 100*accuracy_score(testy, predicty),("%"))

print("Accuracy of the model :",100*modelgnb.score(testx,testy))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative']
print("Classification report:")
print(classification_report(testy,predicty, target_names=target_names))

#Generate the confusion matrix
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
cm=confusion_matrix(testy,predicty)
#Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['Positive','Negative'])
print("Confusion Matrix:")
disp.plot()

"""# Multinomial Naive Bayes"""

# Defining the models and Training them
modelmnb = MultinomialNB(alpha=1.0,fit_prior=True)
print("Multinomial Naive Bayes:")
modelmnb.fit(trainx,trainy)
validpredict = modelmnb.predict(validx)
print("validation accuracy ", 100*accuracy_score(testy,validpredict),("%"))
predicty = modelmnb.predict(testx)
print("test accuracy ", 100*accuracy_score(testy, predicty),("%"))

print("Accuracy of the model :",100*modelmnb.score(testx,testy))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative']
print("Classification report:")
print(classification_report(testy,predicty, target_names=target_names))

#Generate the confusion matrix
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
cm=confusion_matrix(testy,predicty)
#Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['Positive','Negative'])
print("Confusion Matrix:")
disp.plot()

"""# Bernoulli Naive Bayes"""

# Defining the models and Training them
modelbnb = BernoulliNB(alpha=1.0,fit_prior=True)
print("Bernoulli Naive Bayes:")
modelbnb.fit(trainx,trainy)
validpredict = modelbnb.predict(validx)
print("validation accuracy ", 100*accuracy_score(testy,validpredict),("%"))
predicty = modelbnb.predict(testx)
print("test accuracy ", 100*accuracy_score(testy, predicty),("%"))

print("Accuracy of the model :",100*modelbnb.score(testx,testy))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative']
print("Classification report:")
print(classification_report(testy,predicty, target_names=target_names))

#Generate the confusion matrix
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
cm=confusion_matrix(testy,predicty)
#Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['Positive','Negative'])
print("Confusion Matrix:")
disp.plot()

"""# MLP Classifier model - 50 iterations"""

from sklearn.neural_network import MLPClassifier
mlpmodel = MLPClassifier(hidden_layer_sizes=(10,10), activation='relu', solver='adam', 
                                  batch_size= 128, learning_rate='adaptive', max_iter=10, validation_fraction = 0.1, random_state= 441, 
                                  verbose=True).fit(trainx,trainy)
mlpmodel.score(testx, testy)

predicty = mlpmodel.predict(testx)
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
cm=confusion_matrix(testy,predicty)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['Positive','Negative'])
disp.plot()
print("Accuracy of the model :",100*mlpmodel.score(testx,testy))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative']
print(classification_report(testy,predicty, target_names=target_names))

"""#KNN classifier

"""

from sklearn.neighbors import KNeighborsClassifier
knnmodel = KNeighborsClassifier(n_neighbors=5)
knnmodel.fit(trainx,trainy)
print(knnmodel.score(testx, testy)*100,('%'))
#print(knnmodel.predict(review))

predicty = knnmodel.predict(testx)
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
cm=confusion_matrix(testy,predicty)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['Positive','Negative'])
disp.plot()
print("Accuracy of the model :",100*knnmodel.score(testx,testy))

from sklearn.metrics import classification_report
target_names = ['Positive', 'Negative']
print(classification_report(testy,predicty, target_names=target_names))

"""# Saving all the Classifier Models"""

pickle.dump(modelgnb,open('modelgnb.pkl','wb'))
pickle.dump(modelmnb,open('modelmnb.pkl','wb'))
pickle.dump(modelbnb,open('modelbnb.pkl','wb'))
pickle.dump(mlpmodel,open('mlpmodel.pkl','wb'))
pickle.dump(knnmodel,open('knnmodel.pkl','wb'))

"""# Making Predictions!!"""

# review to be predicted - POSITIVE REVIEW

review = """If a person is bit of into movies, I am sure that person heard about Baahubali couple of years back. It took almost four years to complete the first part. Hero Prabhas had completely dedicated his time to this movie and no movie after his super successful movie Mirchi. Same is true from S.S Rajamouly, after his last non-conventional movie Eega. So obviously there are high expectations.

I was eagerly waiting for this movie and within no time I booked the tickets and went with my family. The results are out now, history books are being rewritten. An excellent movie, every frame is a mega scale. I couldn't remember a single scene which I felt bored. This movie will redefine Indian movie standards. Nothing less than The Lord of the Rings or in fact, can easily compete with any Hollywood movie.

Each and every character superbly framed. Every actor has done a decent job and Prabhas and Raana are over the sky, earned more than 100% credit. Can't wait for part 2.

Overall It's a superb visual treat."""

f1 = clean(review)
f2 = is_special(f1)
f3 = to_lower(f2)
f4 = rem_stopwords(f3)
f5 = stem_txt(f4)

bow,words = [],word_tokenize(f5)
for word in words:
    bow.append(words.count(word))
#np.array(bow).reshape(1,3000)
#bow.shape
word_dict = cv.vocabulary_
pickle.dump(word_dict,open('bow.pkl','wb'))
inp = []
for i in word_dict:
    inp.append(f5.count(i[0]))

print("Review:\n",review,"\n\n")

y_pred_gnb = modelgnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Gaussian NB: ',y_pred_gnb)
y_pred_mnb = modelmnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Multinomial NB: ',y_pred_mnb)
y_pred_bnb = modelbnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Bernoulli NB: ', y_pred_bnb)
y_pred_mlp = mlpmodel.predict(np.array(inp).reshape(1,1000))
print('Prediction using MLP: ',y_pred_mlp)
y_pred_knn = knnmodel.predict(np.array(inp).reshape(1,1000))
print('Prediction using KNN: ',y_pred_knn)

#review to be predicted  - NEGATIVE REVIEW

review = """Terrible. Complete trash. Brainless tripe. Insulting to anyone who isn't an 8 year old fan boy. Im actually pretty disgusted that this movie is making the money it is - what does it say about the people who brainlessly hand over the hard earned cash to be 'entertained' in this fashion and then come here to leave a positive 8.8 review?? Oh yes, they are morons. Its the only sensible conclusion to draw. How anyone can rate this movie amongst the pantheon of great titles is beyond me.

So trying to find something constructive to say about this title is hard...I enjoyed Iron Man? Tony Stark is an inspirational character in his own movies but here he is a pale shadow of that...About the only 'hook' this movie had into me was wondering when and if Iron Man would knock Captain America out...Oh how I wished he had :( What were these other characters anyways? Useless, bickering idiots who really couldn't organise happy times in a brewery. The film was a chaotic mish mash of action elements and failed 'set pieces'...

I found the villain to be quite amusing.

And now I give up. This movie is not robbing any more of my time but I felt I ought to contribute to restoring the obvious fake rating and reviews this movie has been getting on IMDb."""

f1 = clean(review)
f2 = is_special(f1)
f3 = to_lower(f2)
f4 = rem_stopwords(f3)
f5 = stem_txt(f4)

bow,words = [],word_tokenize(f5)
for word in words:
    bow.append(words.count(word))
#np.array(bow).reshape(1,3000)
#bow.shape
word_dict = cv.vocabulary_
pickle.dump(word_dict,open('bow.pkl','wb'))
inp = []
for i in word_dict:
    inp.append(f5.count(i[0]))

print("Review:\n",review,"\n\n")

y_pred_gnb = modelgnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Gaussian NB: ',y_pred_gnb)
y_pred_mnb = modelmnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Multinomial NB: ',y_pred_mnb)
y_pred_bnb = modelbnb.predict(np.array(inp).reshape(1,1000))
print('Prediction using Bernoulli NB: ', y_pred_bnb)
y_pred_mlp = mlpmodel.predict(np.array(inp).reshape(1,1000))
print('Prediction using MLP: ',y_pred_mlp)
y_pred_knn = knnmodel.predict(np.array(inp).reshape(1,1000))
print('Prediction using KNN: ',y_pred_knn)